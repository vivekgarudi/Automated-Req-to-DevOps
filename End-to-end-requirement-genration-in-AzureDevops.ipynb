{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c6b138",
   "metadata": {},
   "source": [
    "# Process flow for End to end requirement gather process using openAI and Semantic kernel\n",
    "\n",
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0cb1d",
   "metadata": {},
   "source": [
    "# Folder structure\n",
    "\n",
    "Root\n",
    "-- HistoricalInformation(Contains historical data of features ,description and acceptance criteria of all the previous project which can be used for getting description and acceptance crteria for similar requirement)\n",
    "\n",
    "    -- Feature_data.csv (A CSV storage for past infomration this data can come from any database(SQL,congnitive search,mongoDB). We used csv for reference) \n",
    "\n",
    "-- Skills\n",
    "    -- AzureDevops\n",
    "\n",
    "        --FeatureDescription (semantic function for genrating feature descritption and acceptance crieria based on predefined template)\n",
    "\n",
    "        --IdentifyFeature (semantic function for identify features\\requirement in given text)\n",
    "\n",
    "        --isRequirement (semantic function to check if the string contains requirement)\n",
    "\n",
    "        --native_function.py (Not used)\n",
    "\n",
    "-- Projectdata (this folder includes everything relates to specific project including base, as wele as processed information)\n",
    "\n",
    "    --Project_information (Contains reference information which will used as in addition to the discussion transcript to identify requirement)\n",
    "\n",
    "        --Customer_objective.txt (contains what customer\\company wants to achive by this project)\n",
    "\n",
    "        --NFR.txt (Non functional requirement for the project which supposed to be part of the project even if its not been discussed)\n",
    "\n",
    "        --project_objective.txt (what the objective of project sponsor.)\n",
    "\n",
    "    --Rawtxt (folder where you have to put all the requirement discussion transcripts)\n",
    "\n",
    "    --Requirement_discussion (This folder would be used by script to store finalized sub transcript which is  <3000 char and contains requirements)\n",
    "    \n",
    "    --temp (Teamp folder used for storing all the transcripts which are genrated as part of stranscript splitting process)\n",
    "\n",
    "--End-to-end-requirement-genration-in-AzureDevops.ipynb (Jypyter Notebook stiches everything togather)\n",
    "\n",
    "![Alt text](image-7.png)\n",
    "\n",
    "# Information gathering\n",
    "The output of this steps is all the relevent information which can directly of indirectly contribute to project success.\n",
    "- Get Delivery Objectives (store the information in Root\\Projectdata\\Project_information\\Customer_objective.txt and project_objective.txt)\n",
    "- Gather Non-functional requirements (store the information in Root\\Projectdata\\Project_information\\NFR.txt)\n",
    "- Requirements elaboration session transcripts (Store the transcripts inside Root\\Projectdata\\Rawtxt\\)\n",
    "Once you have this information you can start with document processing by performing below mentined steps\n",
    "\n",
    "\n",
    "# Document processing\n",
    "The objective of this steps is to reduce the size of documents to limited document which actual contains requirement information.\n",
    "The output of this step is te get all the sub transcripts containg requirement in Root\\Projectdata\\Requirement_discussion\\ folder.\n",
    "- Read raw discussion document from \"Rawtxt\" folders.\n",
    "- Breaking big discussion transcript into smaller transcripts of size 3000 character.\n",
    "- Put them in temp folder\n",
    "- Check if subpart of the discussion contains requirement or not.\n",
    "- It contains requirement put that into \"Requirement_discussion\" folder\n",
    "- Import historical feature data in memory\n",
    "Covered in steps 1 to 7\n",
    "![Alt text](image-8.png)\n",
    "\n",
    "Once you have the list of document which contains requirement we can proceed further with feature identification.\n",
    "\n",
    "# Requirement identification and refinment\n",
    "The objective of this process is to identify and groom features and make them ready for importing in to Azure devops\n",
    "The optput of this steps is finalized list of features and there details.\n",
    "- Identify features from Proceed transcript\n",
    "- remove duplicate\n",
    "- Refine ,select or add new feature\n",
    "- groom the selected features\n",
    "- finalize feature details to be imported in Azure devops\n",
    "Covered in steps 7 to 11\n",
    "# Import in Azure devops\n",
    "The objective of this steps is to import features in Azure devops with there respective metadata.\n",
    "covered step 12 and 13\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {
    "tags": [
     "funtion"
    ]
   },
   "source": [
    "## Step 1 \n",
    "Install all python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da651d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install azure-devops\n",
    "!python -m pip install semantic-kernel==0.3.10.dev0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460e218",
   "metadata": {},
   "source": [
    "## Step 2 \n",
    "Import Packages required and instantiate objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd150646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel import ContextVariables # Context to store variables and Kernel to interact with the kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion ,AzureTextEmbedding # AI services\n",
    "import pandas as pd\n",
    "from semantic_kernel.connectors.search_engine import BingConnector\n",
    "from semantic_kernel.core_skills import WebSearchEngineSkill\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import ( AzureCognitiveSearchMemoryStore)\n",
    "import subprocess\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "kernel = sk.Kernel() # Create a kernel instance\n",
    "kernel1 = sk.Kernel() # Create another kernel instance for not having semanitc function in the same kernel \n",
    "\n",
    "useAzureOpenAI = True\n",
    "config = dotenv_values(\".env\")\n",
    "AZURE_OPENAI_API_KEY = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_ENDPOINT = config[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = config[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "AZURE_OPENAI_EM_DEPLOYMENT_NAME = config[\"AZURE_OPENAI_EM_DEPLOYMENT_NAME\"]\n",
    "BING_API_KEY = config[\"BING_API_KEY\"]\n",
    "AZURE_COGNITIVE_SEARCH_ENDPOINT = config[\"AZURE_COGNITIVE_SEARCH_ENDPOINT\"]\n",
    "AZURE_COGNITIVE_SEARCH_ADMIN_KEY = config[\"AZURE_COGNITIVE_SEARCH_ADMIN_KEY\"]\n",
    "vector_size = 1536\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat_completion\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "    kernel1.add_chat_service(\"chat_completion\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "    kernel1.add_text_embedding_generation_service(\"ada\",AzureTextEmbedding(deployment_name=AZURE_OPENAI_EM_DEPLOYMENT_NAME,endpoint=AZURE_OPENAI_ENDPOINT,api_key=AZURE_OPENAI_API_KEY))\n",
    "    connector = BingConnector(api_key=BING_API_KEY)\n",
    "    web_skill = kernel1.import_skill(WebSearchEngineSkill(connector), \"WebSearch\")\n",
    "    search_async = web_skill[\"searchAsync\"]    \n",
    "    kernel1.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "    kernel1.import_skill(sk.core_skills.TextMemorySkill())\n",
    "    COLLECTION_NAME = \"historicalfeaturedata\"\n",
    "    context = kernel1.create_new_context()\n",
    "    context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = COLLECTION_NAME\n",
    "    context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "    # connector = AzureCognitiveSearchMemoryStore(vector_size, AZURE_COGNITIVE_SEARCH_ENDPOINT, AZURE_COGNITIVE_SEARCH_ADMIN_KEY )\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Import the **Semantic functions**:Azuredevops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0183226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: using skills from the samples folder\n",
    "plugins_directory = \"./skills\"\n",
    "# Import the semantic functions\n",
    "DevFunctions=kernel1.import_semantic_skill_from_directory(plugins_directory, \"AzureDevOps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdfeb3",
   "metadata": {},
   "source": [
    "## Step 4 \n",
    "Splitting raw transcripts to files with <3000 char and copy them to Root\\Projectdata\\temp\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3808beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseprojectfoler=(r\"C:\\Users\\vigarudi\\Documents\\code\\Python\\getfeatures\\Projectdata\")\n",
    "rawtxtfolder=feature_list=os.path.join(baseprojectfoler , \"Rawtxt\")\n",
    "Requirement_folder_path = os.path.join(baseprojectfoler , \"Requirement_discussion\")\n",
    "temp_folder_path = os.path.join(baseprojectfoler , \"temp\")\n",
    "# Delete the contents of the folder\n",
    "if os.path.exists(temp_folder_path):\n",
    "    shutil.rmtree(temp_folder_path)\n",
    "os.mkdir(temp_folder_path)\n",
    "max_size = 3000\n",
    "file_list = os.listdir(rawtxtfolder )\n",
    "x=0\n",
    "# Iterate through each file and read its contents\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(rawtxtfolder , filename)  \n",
    "    targetfilepath=os.path.join(temp_folder_path , filename)  \n",
    "    infoid=\"info\"+str(x)\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            num_parts = (len(content) + max_size - 1) // max_size\n",
    "            for i in range(num_parts):\n",
    "                part = content[i*max_size:(i+1)*max_size]\n",
    "                part_path = f\"{targetfilepath}_{i+1}.txt\"\n",
    "                with open(part_path, \"w\") as part_file:\n",
    "                    part_file.write(part)\n",
    "    x=x+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702af8b",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "Identify splitted files which are actually containing requirement using \"isrequirement\" sementic function and copy them to Root\\Projectdata\\Requirement_discussion\\ if they do have requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIdenFunction = DevFunctions[\"isrequirement\"] \n",
    "max_size = 3000\n",
    "file_list = os.listdir(temp_folder_path)\n",
    "x=0\n",
    "# Iterate through each file and read its contents\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(temp_folder_path , filename)  \n",
    "    infoid=\"info\"+str(x)\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            contentf = file.read()\n",
    "            resultFD = FIdenFunction(contentf)\n",
    "            print(file_path +\"   \"+ str(resultFD))\n",
    "            if \"yes\" in str(resultFD).lower() :\n",
    "                shutil.copy(file_path, Requirement_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c33d0",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "Read all project information and set project folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a70298",
   "metadata": {},
   "outputs": [],
   "source": [
    "Requirement_folder_path = (r\"C:\\Users\\vigarudi\\Documents\\code\\Python\\getfeatures\\Projectdata\\Requirement_discussion\")  # Replace with the path to your folder\n",
    "Project_folder_path = (r\"C:\\Users\\vigarudi\\Documents\\code\\Python\\getfeatures\\Projectdata\\Project_information\")  # Replace with the path to your folder\n",
    "feature_list=os.path.join(Project_folder_path , \"featurelist_raw.json\")\n",
    "Risk_list=os.path.join(Project_folder_path , \"Risklist_raw.json\")\n",
    "CustomerObjective= os.path.join(Requirement_folder_path, \"Customer_objective.txt\") # Replace with the path to your folder\n",
    "if os.path.isfile(CustomerObjective):\n",
    "        with open(CustomerObjective, 'r') as file:\n",
    "            #context[\"cprojectgoal\"]  = file.read()\n",
    "            context[\"cprojectgoal\"]  = \"\"\n",
    "ISD_objective= os.path.join(Requirement_folder_path, \"ISD_objective.txt\") # Replace with the path to your folder\n",
    "if os.path.isfile(ISD_objective):\n",
    "        with open(ISD_objective, 'r') as file:\n",
    "            #context[\"iprojectgoal\"]  = file.read()\n",
    "            context[\"iprojectgoal\"]  = \"\"\n",
    "NFR= os.path.join(Requirement_folder_path, \"NFR.txt\") # Replace with the path to your folder\n",
    "if os.path.isfile(NFR):\n",
    "        with open(NFR, 'r') as file:\n",
    "            context[\"nfrtext\"] = \"\"\n",
    "            #context[\"nfrtext\"] = file.read()\n",
    "# List all files in the folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9ca74",
   "metadata": {},
   "source": [
    "## Step 7 \n",
    "Import historical data in memory for using it subsequent steps. If we are using external source such as SQL,COgnitive search, mongodb then this steps would change according to source.\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0812a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "kernel1.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "kernel1.import_skill(sk.core_skills.TextMemorySkill())\n",
    "csv_file_path=r\"C:\\Users\\vigarudi\\Documents\\code\\Python\\getfeatures\\HistoricalInformation\\Feature_data.csv\"\n",
    "with open(csv_file_path, mode='r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            x=0\n",
    "            # Loop through each row in the CSV file\n",
    "            for row in reader:\n",
    "            # Each row is a list of values\n",
    "            # Access values by index, e.g., row[0], row[1], ...        \n",
    "                if row[0] != \"featurename\":\n",
    "                    if row[2] is None or row[2] == \"\"or row[2] == \" \":\n",
    "                        row[2]=\"none\"\n",
    "                    if row[1] is None or row[2] == \"\" or row[2] == \" \":\n",
    "                        row[1]=\"none\"\n",
    "                    txq='{\"featurename\":\"' +row[0]+ '\", \"description\":\"' +row[1] + '\",\"acceptancecriteria\":\"' + row[2] + '\"}'\n",
    "                    infoid=\"info\"+str(x)\n",
    "                    print(txq)\n",
    "                    await kernel1.memory.save_information_async(COLLECTION_NAME , id=infoid, text=txq)\n",
    "                x=x+1   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b669171",
   "metadata": {},
   "source": [
    "## Step 8 \n",
    "We pick files from Root\\Projectdata\\Requirement_discussion\\ folder and process those using \"IdentifyFeatures\" semantic function.Post that we would remove dulicate requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "allfeatures=[]\n",
    "Requirement_folder_path = (r\"C:\\Users\\vigarudi\\Documents\\code\\Python\\getfeatures\\Projectdata\\Requirement_discussion\")  # Replace with the path to your folder\n",
    "allfeatures=[]\n",
    "FIdenFunction = DevFunctions[\"IdentifyFeatures\"] \n",
    "file_list = os.listdir(Requirement_folder_path )\n",
    "x=1\n",
    "# Iterate through each file and read its contents\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(Requirement_folder_path , filename)    \n",
    "    infoid=\"info\"+str(x)\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            context[\"requirementtext\"] = file.read()\n",
    "            if len(allfeatures ) != 0:\n",
    "                #context[\"featurelist\"] = str(allfeatures)\n",
    "                context[\"featurelist\"] = \"\"\n",
    "            resultIF = FIdenFunction(context=context)\n",
    "            print(resultIF)\n",
    "            jsonIF = json.loads(str(resultIF))\n",
    "            print(jsonIF)\n",
    "            for feature in jsonIF:\n",
    "                allfeatures.append({\"featureid\": x,\"featurename\": feature[\"feature\"],\"descriptions\":[{\"descriptionid\": 1,\"description\":feature[\"description\"],\"acceptancecriteria\":\"\",\"approved\":\"n\",\"source\":\"Identify feature\"}],\"discussionname\":file_path,\"approved\":\"n\"})\n",
    "                x=x+1\n",
    "            # Now, you can work with the file_contents as a string\n",
    "            #print(filename , file_contents)\n",
    "            #await kernel1.memory.save_information_async(COLLECTION_NAME, id=infoid, text=context[\"requirementtext\"])          \n",
    "\n",
    "#Remove duplicate requirement\n",
    "unique_array = []\n",
    "names = set()\n",
    "for obj in allfeatures:\n",
    "    name = obj.get(\"featurename\")\n",
    "    if name is not None and name.lower() not in names:\n",
    "        unique_array.append(obj)\n",
    "        names.add(name.lower())\n",
    "\n",
    "with open(feature_list, \"w\") as f:\n",
    "    json.dump(unique_array, f)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12aa80c",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "We would perform manual refinment of requirement by updating details in base feature list Json \"\\Projectdata\\Project_information\\featurelist_raw.json\" In this step we can update feature title ,description and select the feature which are required to implement project by changing the value of \"approved\" tag to \"y\". We can manually add feature if its been missed in list.\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "## Step 10\n",
    "\n",
    "Once you have finalized the list of feature, we groom these feature using \"FeatureDescription\" semantic function and historical data from memory.The outcome of this step would be, addition of multiple option of description and acceptance criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "allfeatures=[]\n",
    "FDesFunction = DevFunctions[\"FeatureDescription\"] \n",
    "# Iterate through each file and read its contents\n",
    "with open(feature_list, \"r\") as f:\n",
    "    allfeatures1 = json.load(f)\n",
    "for feature in allfeatures1:\n",
    "    print(feature[\"approved\"] +\"  \"+ feature[\"featurename\"])\n",
    "    if feature[\"approved\"] == \"y\" or feature[\"approved\"] == \"Y\":\n",
    "            context[\"title\"] =feature[\"featurename\"]\n",
    "            context[\"description\"] =feature[\"descriptions\"][0][\"description\"]\n",
    "            print(context[\"description\"])\n",
    "            print(context[\"title\"])\n",
    "            resultFD = FDesFunction(context=context)\n",
    "            print(resultFD)\n",
    "            jsonIF = json.loads(str(resultFD))\n",
    "            feature[\"descriptions\"].append({\"descriptionid\": 2,\"description\":jsonIF[\"description\"],\"acceptancecriteria\":jsonIF[\"acceptancecriteria\"],\"approved\":\"y\",\"source\":\"FeatureDescription\"})\n",
    "            # add historical description\n",
    "            historicfeature= await kernel1.memory.search_async(COLLECTION_NAME, feature[\"featurename\"])\n",
    "            txtquery=historicfeature[0].text\n",
    "            if historicfeature is not None:\n",
    "                x=3\n",
    "                for featuredata in historicfeature:\n",
    "                    print(featuredata.text)\n",
    "                    hsjson = json.loads(featuredata.text)\n",
    "                    print(hsjson[\"featurename\"])\n",
    "                    feature[\"descriptions\"].append({\"descriptionid\": x,\"description\":hsjson[\"description\"],\"acceptancecriteria\":hsjson[\"acceptancecriteria\"],\"approved\":\"n\",\"source\":\"historicaldata\"})\n",
    "                    x=x+1\n",
    "            allfeatures.append({\"featurename\": jsonIF[\"featurename\"],\"descriptions\":feature[\"descriptions\"],\"discussionname\":feature[\"discussionname\"],\"approved\":feature[\"approved\"]})\n",
    "    else:\n",
    "        allfeatures.append(feature)\n",
    "              # Now, you can work with the file_contents as a string\n",
    "            #print(filename , file_contents)\n",
    "            #await kernel1.memory.save_information_async(COLLECTION_NAME, id=infoid, text=context[\"requirementtext\"])\n",
    "            \n",
    "    x=x+1       \n",
    "    \n",
    "\n",
    "with open(feature_list, \"w\") as f:\n",
    "    json.dump(allfeatures, f)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34225b9b",
   "metadata": {},
   "source": [
    "## Step 11\n",
    "This is the final step befor we import these features in Azure DevOps. In this steps,wWe would perform manual refinment of requirement by updating details in feature list Json \"\\Projectdata\\Project_information\\featurelist_raw.json\". In this step, we can update feature title ,description and select the Description and acceptance criteria from \"descriptions\" array by changing the value of \"approved\" tag to \"y\". We can manually add\\update feature description if its been missed in list.\n",
    "\n",
    "![Alt text](image-4.png)\n",
    "\n",
    "## Step 12\n",
    "\n",
    "Once we have finalized list of features,description ,acceptance crieria(you can inlude other parameters such as efforts business value priority etc as well). we can extecute below step to import them in Azure devops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d3a5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.devops.v7_1.py_pi_api import JsonPatchOperation\n",
    "from azure.devops.connection import Connection\n",
    "from msrest.authentication import BasicAuthentication\n",
    "from azure.devops.v7_1.work_item_tracking.models import Wiql\n",
    "import base64\n",
    "from semantic_kernel import ContextVariables, Kernel\n",
    "import re\n",
    "import time\n",
    "feature_title = \"test\"\n",
    "description=\"description\"\n",
    "acceptancecriteria=\"\"\n",
    "targetOrganizationName= \"vigarudi0944\"\n",
    "targetProjectName= \"test\"\n",
    "#targetOrganizationPAT = \"i3i2vw5scgscy6hn26wswrdjjowmicnrzlyi7gy5o2doofto2vsa\"\n",
    "targetOrganizationPAT = \"cdleu3tixtj5i4kg27yy7dnm3sdmsjxjhfy5aeh3wpk5kkvjowpa\"\n",
    "teamName = \"test Team\"\n",
    "areaName = teamName\n",
    "iterationName =\"Sprint 1\"\n",
    "targetOrganizationUri='https://dev.azure.com/'+targetOrganizationName\n",
    "credentials = BasicAuthentication('', targetOrganizationPAT)\n",
    "connection = Connection(base_url=targetOrganizationUri, creds=credentials)\n",
    "userToken = \"\" + \":\" + targetOrganizationPAT\n",
    "base64UserToken = base64.b64encode(userToken.encode()).decode()\n",
    "headers = {'Authorization': 'Basic' + base64UserToken}\n",
    "core_client = connection.clients.get_core_client()\n",
    "targetProjectId = core_client.get_project(targetProjectName).id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2d843",
   "metadata": {},
   "source": [
    "Lets import it in Azure devops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19f387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49cae7e9-144b-481c-afbb-bc6b5260c3a1\n",
      "Error occurred in request., ConnectionError: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Error occurred in request., ConnectionError: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "{'additional_properties': {}, 'url': 'https://dev.azure.com/vigarudi0944/49cae7e9-144b-481c-afbb-bc6b5260c3a1/_apis/wit/workItems/247', '_links': <azure.devops.v7_0.work_item_tracking.models.ReferenceLinks object at 0x000001E77D482650>, 'comment_version_ref': None, 'fields': {'System.AreaPath': 'test', 'System.TeamProject': 'test', 'System.IterationPath': 'test\\\\Sprint 1', 'System.WorkItemType': 'Feature', 'System.State': 'New', 'System.Reason': 'New feature', 'System.CreatedDate': '2023-10-25T22:30:03.34Z', 'System.CreatedBy': {'displayName': 'vivek', 'url': 'https://spsprodcus5.vssps.visualstudio.com/A36f579da-534a-44ad-b859-950ff2c74476/_apis/Identities/017fe82e-1ad9-63a5-bef7-b4084a7a493e', '_links': {'avatar': {'href': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}}, 'id': '017fe82e-1ad9-63a5-bef7-b4084a7a493e', 'uniqueName': 'vigarudi@microsoft.com', 'imageUrl': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl', 'descriptor': 'aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}, 'System.ChangedDate': '2023-10-25T22:30:03.34Z', 'System.ChangedBy': {'displayName': 'vivek', 'url': 'https://spsprodcus5.vssps.visualstudio.com/A36f579da-534a-44ad-b859-950ff2c74476/_apis/Identities/017fe82e-1ad9-63a5-bef7-b4084a7a493e', '_links': {'avatar': {'href': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}}, 'id': '017fe82e-1ad9-63a5-bef7-b4084a7a493e', 'uniqueName': 'vigarudi@microsoft.com', 'imageUrl': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl', 'descriptor': 'aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}, 'System.CommentCount': 0, 'System.Title': 'OpenAI and Semantic Kernels Integration', 'Microsoft.VSTS.Common.StateChangeDate': '2023-10-25T22:30:03.34Z', 'Microsoft.VSTS.Common.Priority': 2, 'Microsoft.VSTS.Common.ValueArea': 'Business', 'System.Description': 'Leverage the power of OpenAI and semantic kernels to create a more efficient and accurate requirements gathering tool. This feature aims to address the need for a more precise and efficient tool for gathering requirements. <br><br>User Needs- <br>- More accurate requirements gathering <br>- Increased efficiency in requirements gathering <br><br>Functional Requirements-<br>- Integration of OpenAI <br>- Integration of semantic kernels <br>- Creation of a requirements gathering tool <br><br>Non-Functional Requirements-<br>- User-friendly interface <br>- High performance <br>- Reliable and secure <br><br>Feature Scope <br>This feature should be able to integrate OpenAI and semantic kernels to create a requirements gathering tool that is more accurate and efficient.', 'Microsoft.VSTS.Common.AcceptanceCriteria': 'The feature is considered complete when: <br>- OpenAI is successfully integrated <br>- Semantic kernels are successfully integrated <br>- The requirements gathering tool is created and functions as expected <br><br>Feature Scope <br>This feature should be able to integrate OpenAI and semantic kernels to create a requirements gathering tool that is more accurate and efficient.'}, 'id': 247, 'relations': None, 'rev': 1}\n",
      "49cae7e9-144b-481c-afbb-bc6b5260c3a1\n",
      "{'additional_properties': {}, 'url': 'https://dev.azure.com/vigarudi0944/49cae7e9-144b-481c-afbb-bc6b5260c3a1/_apis/wit/workItems/248', '_links': <azure.devops.v7_0.work_item_tracking.models.ReferenceLinks object at 0x000001E77D4769D0>, 'comment_version_ref': None, 'fields': {'System.AreaPath': 'test', 'System.TeamProject': 'test', 'System.IterationPath': 'test\\\\Sprint 1', 'System.WorkItemType': 'Feature', 'System.State': 'New', 'System.Reason': 'New feature', 'System.CreatedDate': '2023-10-25T22:30:08.457Z', 'System.CreatedBy': {'displayName': 'vivek', 'url': 'https://spsprodcus5.vssps.visualstudio.com/A36f579da-534a-44ad-b859-950ff2c74476/_apis/Identities/017fe82e-1ad9-63a5-bef7-b4084a7a493e', '_links': {'avatar': {'href': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}}, 'id': '017fe82e-1ad9-63a5-bef7-b4084a7a493e', 'uniqueName': 'vigarudi@microsoft.com', 'imageUrl': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl', 'descriptor': 'aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}, 'System.ChangedDate': '2023-10-25T22:30:08.457Z', 'System.ChangedBy': {'displayName': 'vivek', 'url': 'https://spsprodcus5.vssps.visualstudio.com/A36f579da-534a-44ad-b859-950ff2c74476/_apis/Identities/017fe82e-1ad9-63a5-bef7-b4084a7a493e', '_links': {'avatar': {'href': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}}, 'id': '017fe82e-1ad9-63a5-bef7-b4084a7a493e', 'uniqueName': 'vigarudi@microsoft.com', 'imageUrl': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl', 'descriptor': 'aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}, 'System.CommentCount': 0, 'System.Title': 'Regular Security Audits', 'Microsoft.VSTS.Common.StateChangeDate': '2023-10-25T22:30:08.457Z', 'Microsoft.VSTS.Common.Priority': 2, 'Microsoft.VSTS.Common.ValueArea': 'Business', 'System.Description': 'Regularly audit and update our security measures to stay ahead of potential threats. This feature is crucial to ensure the safety and integrity of our product or project. <br> User Needs- <br> Users need to feel secure and trust that their data is protected. Regular security audits will help maintain this trust. <br> <br>Functional Requirements-<br>- Ability to schedule regular security audits <br>- Update security measures based on audit findings <br>- Notify relevant parties of audit results and updates <br> <br>Non-Functional Requirements-<br>- The audit process should be efficient and not disrupt normal operations <br>- The audit results should be clear and easy to understand <br>- The updates should be implemented swiftly and effectively <br>Feature Scope <br>This feature should at least be able to schedule audits, perform audits, and implement updates based on the audit findings.', 'Microsoft.VSTS.Common.AcceptanceCriteria': 'The feature is considered complete when it can: <br>- Schedule regular security audits <br>- Perform these audits effectively <br>- Implement updates based on the audit findings <br> <br>Feature Scope <br>This feature should at least be able to schedule audits, perform audits, and implement updates based on the audit findings.'}, 'id': 248, 'relations': None, 'rev': 1}\n",
      "49cae7e9-144b-481c-afbb-bc6b5260c3a1\n",
      "{'additional_properties': {}, 'url': 'https://dev.azure.com/vigarudi0944/49cae7e9-144b-481c-afbb-bc6b5260c3a1/_apis/wit/workItems/249', '_links': <azure.devops.v7_0.work_item_tracking.models.ReferenceLinks object at 0x000001E77D482410>, 'comment_version_ref': None, 'fields': {'System.AreaPath': 'test', 'System.TeamProject': 'test', 'System.IterationPath': 'test\\\\Sprint 1', 'System.WorkItemType': 'Feature', 'System.State': 'New', 'System.Reason': 'New feature', 'System.CreatedDate': '2023-10-25T22:30:13.557Z', 'System.CreatedBy': {'displayName': 'vivek', 'url': 'https://spsprodcus5.vssps.visualstudio.com/A36f579da-534a-44ad-b859-950ff2c74476/_apis/Identities/017fe82e-1ad9-63a5-bef7-b4084a7a493e', '_links': {'avatar': {'href': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}}, 'id': '017fe82e-1ad9-63a5-bef7-b4084a7a493e', 'uniqueName': 'vigarudi@microsoft.com', 'imageUrl': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl', 'descriptor': 'aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}, 'System.ChangedDate': '2023-10-25T22:30:13.557Z', 'System.ChangedBy': {'displayName': 'vivek', 'url': 'https://spsprodcus5.vssps.visualstudio.com/A36f579da-534a-44ad-b859-950ff2c74476/_apis/Identities/017fe82e-1ad9-63a5-bef7-b4084a7a493e', '_links': {'avatar': {'href': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}}, 'id': '017fe82e-1ad9-63a5-bef7-b4084a7a493e', 'uniqueName': 'vigarudi@microsoft.com', 'imageUrl': 'https://dev.azure.com/vigarudi0944/_apis/GraphProfile/MemberAvatars/aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl', 'descriptor': 'aad.MDE3ZmU4MmUtMWFkOS03M2E1LWJlZjctYjQwODRhN2E0OTNl'}, 'System.CommentCount': 0, 'System.Title': 'User-friendly Interface', 'Microsoft.VSTS.Common.StateChangeDate': '2023-10-25T22:30:13.557Z', 'Microsoft.VSTS.Common.Priority': 2, 'Microsoft.VSTS.Common.ValueArea': 'Business', 'System.Description': \"Design the tool with a clean and intuitive user interface. This feature aims to address the user's need for a simple, easy-to-navigate interface that enhances their experience and productivity. <br> User Needs- <br> - Easy navigation <br> - Clear layout <br> - Intuitive design <br> <br>Functional Requirements-<br>- Requirement 1: The interface should be clean and uncluttered <br>- Requirement 2: The design should be intuitive, with clear labels and instructions <br>- Requirement 3: The interface should be responsive and fast <br> <br>Non-Functional Requirements-<br>- Requirement 1: The interface should be visually appealing <br>- Requirement 2: The interface should be consistent across all pages <br>- Requirement 3: The interface should be accessible to users with disabilities <br>Feature Scope <br>This feature should address all the above requirements to provide a user-friendly interface.\", 'Microsoft.VSTS.Common.AcceptanceCriteria': 'The feature will be considered complete if it meets the following criteria: <br>- Criteria 1: The interface is clean and uncluttered <br>- Criteria 2: The design is intuitive, with clear labels and instructions <br>- Criteria 3: The interface is responsive and fast <br>- Criteria 4: The interface is visually appealing <br>- Criteria 5: The interface is consistent across all pages <br>- Criteria 6: The interface is accessible to users with disabilities <br> <br>Feature Scope <br>This feature should address all the above criteria to provide a user-friendly interface.'}, 'id': 249, 'relations': None, 'rev': 1}\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "with open(feature_list, \"r\") as f:\n",
    "    allfeatures = json.load(f)\n",
    "for feature in allfeatures:\n",
    "    if feature[\"approved\"] == \"y\" or feature[\"approved\"] == \"Y\":\n",
    "        for description1 in feature[\"descriptions\"]:\n",
    "            if description1[\"approved\"] == \"y\" or description1[\"approved\"] == \"Y\":\n",
    "                description=description1[\"description\"]     \n",
    "                acceptancecriteria=description1[\"acceptancecriteria\"]   \n",
    "        description = description.replace(\"\\n\", \"<br>\")\n",
    "        acceptancecriteria = acceptancecriteria.replace(\"\\n\", \"<br>\")\n",
    "        #print(description.encode('utf-8'))\n",
    "        \n",
    "        workItemObjects = [\n",
    "        {\n",
    "            'op': 'add',\n",
    "            'path': '/fields/System.WorkItemType',\n",
    "            'value': \"Feature\"\n",
    "        },\n",
    "        {\n",
    "            'op': 'add',\n",
    "            'path': '/fields/System.Title',\n",
    "            'value': feature[\"featurename\"]\n",
    "        },\n",
    "        {\n",
    "            'op': 'add',\n",
    "            'path': '/fields/System.State',\n",
    "            'value': \"New\"\n",
    "        },\n",
    "        {\n",
    "            'op': 'add',\n",
    "            'path': '/fields/System.Description',\n",
    "            'value': description\n",
    "        },\n",
    "        {\n",
    "            'op': 'add',\n",
    "            'path': '/fields/Microsoft.VSTS.Common.AcceptanceCriteria',\n",
    "            'value': acceptancecriteria\n",
    "        },      \n",
    "        {\n",
    "            'op': 'add',\n",
    "            'path': '/fields/System.IterationPath',\n",
    "            'value': targetProjectName+\"\\\\\"+iterationName\n",
    "        }\n",
    "            ]\n",
    "        #for feature in work_client:\n",
    "        #    if feature['path'] == '/fields/System.Description':\n",
    "        #        print(feature['value'])            \n",
    "        jsonPatchList = JsonPatchOperation(workItemObjects)\n",
    "        print(targetProjectId)\n",
    "        work_client = connection.clients.get_work_item_tracking_client()   \n",
    "        max_attempts = 10  # Maximum number of attempts\n",
    "        current_attempt = 0\n",
    "        while current_attempt < max_attempts:\n",
    "            time.sleep(5)\n",
    "            errorvar=None\n",
    "            try:\n",
    "                Response = work_client.create_work_item(jsonPatchList.from_, targetProjectName, \"Feature\")\n",
    "                print(Response)\n",
    "            except Exception as e:\n",
    "                errorvar=str(e)\n",
    "                print(errorvar)\n",
    "            if errorvar is None:\n",
    "                break \n",
    "            current_attempt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899bf796",
   "metadata": {},
   "source": [
    "![Alt text](image-6.png)\n",
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allfeatures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
